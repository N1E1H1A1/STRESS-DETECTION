# -*- coding: utf-8 -*-
"""minor implementaion (c).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JZlngIleEWuSUV8hWOdYzNbE6GHo4d1i
"""

# BERT + FEW-SHOT STRESS DETECTION (Final Clean Version)
# ============================================================

# ---- Step 0: Install compatible packages ----
!pip install torch torchvision torchaudio --quiet
!pip install transformers==4.36.2 tokenizers==0.15.0 --quiet
!pip install scikit-learn pandas --quiet

# ---- Step 1: Imports ----
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, BertModel
from torch.optim import AdamW
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
import pandas as pd
import numpy as np
import random

# ---- Step 2: Setup ----
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# ---- Step 3: Load Dataset ----
train_df = pd.read_csv("/content/dreaddit-train.csv")

# We'll use only the training file for few-shot sampling
df = train_df.copy().reset_index(drop=True)
texts = df["text"].tolist()
labels = df["label"].astype("category").cat.codes.tolist()  # encode labels numerically

# Few-shot sampling (same as before, balanced subset)
random.seed(42)
fewshot_texts, fewshot_labels = [], []
for lbl in np.unique(labels):
    idxs = [i for i, l in enumerate(labels) if l == lbl]
    sampled = random.sample(idxs, min(30, len(idxs)))   # ~30 per class works well
    fewshot_texts += [texts[i] for i in sampled]
    fewshot_labels += [labels[i] for i in sampled]

# No separate validation; use same few-shot set for testing
train_texts, val_texts = fewshot_texts, fewshot_texts
train_labels, val_labels = fewshot_labels, fewshot_labels

# ---- Step 5: Tokenizer + Dataset ----
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
max_len = 128

class FewShotDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len=128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        inputs = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_len,
            padding="max_length",
            truncation=True,
            return_tensors="pt",
        )
        return (
            inputs["input_ids"].squeeze(),
            inputs["attention_mask"].squeeze(),
            torch.tensor(self.labels[idx], dtype=torch.long),
        )

batch_size = 4
train_loader = DataLoader(FewShotDataset(train_texts, train_labels, tokenizer, max_len), batch_size=batch_size, shuffle=True)
val_loader   = DataLoader(FewShotDataset(val_texts, val_labels, tokenizer, max_len),   batch_size=batch_size, shuffle=False)

# ---- Step 6: Model ----
class BERTClassifier(nn.Module):
    def __init__(self, num_classes):
        super().__init__()
        self.bert = BertModel.from_pretrained("bert-base-uncased")
        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)

    def forward(self, input_ids, attention_mask):
        out = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        cls_token = out.last_hidden_state[:, 0, :]
        logits = self.fc(cls_token)
        return logits

num_classes = len(set(labels))
model = BERTClassifier(num_classes=num_classes).to(device)

# ---- Step 7: Training ----
optimizer = AdamW(model.parameters(), lr=2e-5)
criterion = nn.CrossEntropyLoss()
num_epochs = 3

for epoch in range(num_epochs):
    model.train()
    total_loss = 0
    for input_ids, attention_mask, lbls in train_loader:
        input_ids, attention_mask, lbls = input_ids.to(device), attention_mask.to(device), lbls.to(device)
        optimizer.zero_grad()
        outputs = model(input_ids, attention_mask)
        loss = criterion(outputs, lbls)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    print(f"Epoch {epoch+1}/{num_epochs} - Loss: {total_loss/len(train_loader):.4f}")

# ---- Step 8: Evaluation ----
model.eval()
all_preds, all_labels = [], []

with torch.no_grad():
    for input_ids, attention_mask, lbls in val_loader:
        input_ids, attention_mask, lbls = input_ids.to(device), attention_mask.to(device), lbls.to(device)
        outputs = model(input_ids, attention_mask)
        preds = torch.argmax(outputs, dim=1)
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(lbls.cpu().numpy())

acc  = accuracy_score(all_labels, all_preds)
f1   = f1_score(all_labels, all_preds, average="weighted")
prec = precision_score(all_labels, all_preds, average="weighted")
rec  = recall_score(all_labels, all_preds, average="weighted")

print("\n=== FEW-SHOT RESULTS ===")
print(f"Few-shot Accuracy:  {acc:.4f}")
print(f"Few-shot F1:        {f1:.4f}")
print(f"Few-shot Precision: {prec:.4f}")
print(f"Few-shot Recall:    {rec:.4f}")

