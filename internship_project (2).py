# -*- coding: utf-8 -*-
"""internship project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BYq5Fl44m0_AknY_x01fkmMwWxX-EdEj
"""

# üß† Multi-Class Stress Detection Project
# =====================================================

# ‚úÖ Step 0: Install missing libraries
!pip install catboost xgboost --quiet

# ‚úÖ Step 1: Import Required Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier
from sklearn.svm import SVC
import joblib

# ‚úÖ Step 2: Load Dataset
# =====================================================
df = pd.read_csv('/content/Student Stress Factors (2).csv')

print("Shape of dataset:", df.shape)
print("\nColumns:\n", df.columns)
print("\nMissing values:\n", df.isnull().sum())

# ‚úÖ Step 3: Cleaning & Encoding
# =====================================================
df.dropna(inplace=True)

cat_cols = df.select_dtypes(include=['object']).columns
le = LabelEncoder()
for c in cat_cols:
    df[c] = le.fit_transform(df[c])

print("\nEncoded dataset preview:\n", df.head())

# Check actual column names and data
print(df.columns.tolist())
print("\nSample rows:\n")
print(df.head())

# =====================================================
# ‚úÖ Step 4: Split Features and Target (Fixed)
# =====================================================
target_col = 'How would you rate your stress levels?'

X = df.drop(target_col, axis=1)
y = df[target_col]

# Shift labels to start from 0 for XGBoost compatibility
y = y - 1

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(8,5))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
plt.title("Feature Correlation Heatmap")
plt.show()

sns.countplot(x='How would you rate your stress levels?', data=df, palette='magma')
plt.title("Distribution of Stress Levels among Students")
plt.show()

#logistic regression

from sklearn.linear_model import LogisticRegression

# ‚úÖ Step 5: Updated Model List (Adding Logistic Regression)
# =====================================================
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000, random_state=42), # Naya Addition
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "Random Forest": RandomForestClassifier(n_estimators=200, random_state=42),
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42),

}

results = {}

# Loop wahi purana chalega
for name, model in models.items():
    model.fit(X_train, y_train) # Training
    preds = model.predict(X_test) # Testing
    acc = accuracy_score(y_test, preds)
    results[name] = acc

    print("-" * 30)
    print(f"üèÜ {name} Accuracy: {acc*100:.2f}%")
    print(classification_report(y_test, preds))

    # Confusion Matrix Visualization
    cm = confusion_matrix(y_test, preds)
    plt.figure(figsize=(6,4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f'Confusion Matrix - {name}')
    plt.ylabel('Actual Label')
    plt.xlabel('Predicted Label')
    plt.show()

from sklearn.model_selection import GridSearchCV

# 1. Model define karo
xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)

# 2. Options (Grid) banao ki kya-kya try karna hai
param_grid = {
    'n_estimators': [100, 200],      # Kitne trees banane hain
    'max_depth': [3, 5, 6],           # Tree kitna gehra jayega (Overfitting rokne ke liye)
    'learning_rate': [0.01, 0.1, 0.2] # Seekhne ki raftar
}

# 3. GridSearchCV setup (cv=3 matlab 3-fold cross validation karega)
grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=3, scoring='accuracy', verbose=1)

# 4. Best combination dhoondo
grid_search.fit(X_train, y_train)

# 5. Results dekho
print(f"‚úÖ Best Settings: {grid_search.best_params_}")
print(f"üöÄ Best Accuracy after Tuning: {grid_search.best_score_*100:.2f}%")

# yahia overfiting chekc krhe h like model trining vs testing pe kese perfomr krha h
# XGBoost ke liye check karte hain
best_model = models["XGBoost"]

# 1. Training data par predict
train_preds = best_model.predict(X_train)
train_acc = accuracy_score(y_train, train_preds)

# 2. Testing data par predict  (Jo humne pehle kiya tha)
test_preds = best_model.predict(X_test)
test_acc = accuracy_score(y_test, test_preds)

print(f"üìà Training Accuracy: {train_acc*100:.2f}%")
print(f"üìâ Testing Accuracy: {test_acc*100:.2f}%")







